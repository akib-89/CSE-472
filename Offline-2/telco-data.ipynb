{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the Datasets\n",
    "\n",
    "We will use the pandas library to read the datasets. We will use the `read_csv` method to read the csv files in python and assign it to a dataframe object.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "- pandas\n",
    "- matplotlib\n",
    "- numpy\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "To install the libraries, you can use `pip` or `conda` as follows:\n",
    "\n",
    "```bash\n",
    "pip install pandas matplotlib scikit-learn numpy\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda install pandas matplotlib scikit-learn numpy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "This dataset can be found on the following [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n",
    "\n",
    "The dataset contains 7043 rows and 21 columns.\n",
    "\n",
    "The dataset contains information about a telecom company that has a problem with the churn rate of its customers. The company wants to know which customers are likely to churn next month. The dataset contains information about:\n",
    "\n",
    "- Customers who left within the last month – the column is called Churn\n",
    "- Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "- Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "- Demographic info about the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data\n",
    "def load_data(file_name):\n",
    "    # load dataset\n",
    "    dataset = pd.read_csv(file_name)\n",
    "    \n",
    "    # drop the customerID column\n",
    "    dataset.drop('customerID', axis = 1, inplace = True)\n",
    "    \n",
    "    # replace the cells with single and extra spaces with a Nan value\n",
    "    dataset.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "    \n",
    "    # convert the TotalCharges column to numeric\n",
    "    dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'])\n",
    "    \n",
    "    # convert the dataset to dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_data('Telco-Customer-Churn.csv')\n",
    "\n",
    "# How many rows and columns are in this dataset?\n",
    "print(dataset.shape)\n",
    "\n",
    "# Store the column of data types in a list called data_types\n",
    "data_types = dataset.dtypes\n",
    "\n",
    "# Print the data type of each column\n",
    "print(data_types)\n",
    "\n",
    "# What is the first entry in the dataset?\n",
    "print(dataset.head(2))\n",
    "\n",
    "# What is the last entry in the dataset?\n",
    "print(dataset.tail(2))\n",
    "\n",
    "# What is the name of each column?\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "\n",
    "We will check for missing values in the dataset. If there are any missing values, we will replace them with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()   # check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the missing columns\n",
    "def find_missing_columns(dataset):\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    # Store the column number of the columns with missing values in a list called missing_cols\n",
    "    missing_cols = [i for i in range(dataset_columns_length) if dataset.iloc[:, i].isnull().any()]\n",
    "    \n",
    "    # Print columns index and names with missing values and the number of missing values\n",
    "    for i in missing_cols:\n",
    "        print(i, dataset.columns[i], dataset.iloc[:, i].isnull().sum())\n",
    "            \n",
    "    return missing_cols\n",
    "\n",
    "# Store the column number of the columns with missing values in a list called missing_cols\n",
    "missing_cols = find_missing_columns(dataset)\n",
    "\n",
    "print(missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_missing_values(dataset, missing_columns):\n",
    "    for column in missing_columns:\n",
    "        column_name = dataset.columns[column]\n",
    "        if dataset[column_name].dtype == 'object' or dataset[column_name].dtype == 'int64':\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            dataset[column_name] = imputer.fit_transform(dataset[column_name].values.reshape(-1, 1)).ravel()\n",
    "        elif dataset[column_name].dtype == 'float64':\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            dataset[column_name] = imputer.fit_transform(dataset[column_name].values.reshape(-1, 1)).ravel()\n",
    "    return dataset\n",
    "\n",
    "dataset = impute_missing_values(dataset, missing_cols)\n",
    "\n",
    "missing_cols = find_missing_columns(dataset)\n",
    "print(missing_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the data\n",
    "\n",
    "We will scale the data using the `minmax_scale` method from the `sklearn.preprocessing` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_numerical_values(dataset, column_names_to_scale):\n",
    "    # Scale the numerical values\n",
    "    for column in column_names_to_scale:\n",
    "        dataset[column] = (dataset[column] - dataset[column].min())/(dataset[column].max() - dataset[column].min())\n",
    "    return dataset\n",
    "\n",
    "# Scale the numerical values\n",
    "column_names_to_scale = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "dataset = scale_numerical_values(dataset, column_names_to_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the yes and no values to 1 and 0\n",
    "\n",
    "We first will scan for those columns that have yes and no values and convert them to 1 and 0 respectively.\n",
    "\n",
    "**Note:** This cell is dependent on the dataset and the column names. If you are using a different dataset, you will need to change the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the column that needs to be encoded in 0s and 1s\n",
    "column_name = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "\n",
    "# function to convert the yes and no values to 1 and 0\n",
    "def convert_yes_no(dataset, column_name_to_convert):\n",
    "    # create a label encoder object\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Apply label encoder on the column and replace the column with the encoded values\n",
    "    dataset[column_name_to_convert] = dataset[column_name_to_convert].apply(label_encoder.fit_transform)\n",
    "    \n",
    "    # convert the dataset to a dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = convert_yes_no(dataset, column_name)\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "# print(dataset.head(5))\n",
    "\n",
    "print(dataset.dtypes)\n",
    "print(dataset.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the categorical values to numerical values\n",
    "\n",
    "Now we will convert the categorical values to numerical values. For this purpose we will use the `OneHotEncoder` class from the `sklearn.preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the column that is a categorical variable\n",
    "categorical_col_names = ['Gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "\n",
    "# function to convert the categorical values to numerical values using one hot encoding\n",
    "def convert_categorical(dataset, column_names_to_convert):\n",
    "    column_index = []\n",
    "    \n",
    "    # find the index of the categorical columns\n",
    "    for column_name in column_names_to_convert:\n",
    "        column_index.append(dataset.columns.get_loc(column_name))\n",
    "        \n",
    "    # convert the dataset to a numpy array\n",
    "    dataset_array = dataset.values\n",
    "    \n",
    "    # one hot encoder object \n",
    "    one_hot_encoder = OneHotEncoder(dtype=np.int64, handle_unknown='ignore')\n",
    "\n",
    "    # apply the one hot encoder object on the independent variable dataset\n",
    "    encoded_x = one_hot_encoder.fit_transform(dataset_array[:, column_index]).toarray()\n",
    "    \n",
    "    # drop the original column from the dataset\n",
    "    dataset_array = np.delete(dataset_array, column_index, axis = 1)\n",
    "    \n",
    "    # add the new columns to the dataset\n",
    "    dataset_array = np.concatenate((dataset_array, encoded_x), axis = 1)\n",
    "    \n",
    "    \n",
    "    # get the column names of the new columns\n",
    "    encoded_x_column_names = one_hot_encoder.get_feature_names_out(input_features=column_names_to_convert)\n",
    "    \n",
    "    # drop the old column from the dataset\n",
    "    dataset = dataset.drop(column_names_to_convert, axis = 1)\n",
    "    \n",
    "    # record the data types of each column\n",
    "    original_data_types = dataset.dtypes.to_dict()\n",
    "    # all the data types are int64 for encoded columns\n",
    "    \n",
    "    \n",
    "    # record the last column number of the dataset\n",
    "    last_column_number = len(dataset.columns)\n",
    "    \n",
    "    # reconstruct the new dataset column names\n",
    "    new_column_names = list(dataset.columns[0:last_column_number-1]) + list(encoded_x_column_names)\n",
    "    new_column_names.append(dataset.columns[last_column_number-1])\n",
    "    \n",
    "    # rearrange the columns of the dataset_array \n",
    "    # i.e. bring the dataset_array column with the last column number to the last column number of the new dataset_array\n",
    "    dataset_array = np.concatenate((dataset_array[:, 0:last_column_number-1], dataset_array[:, last_column_number:], dataset_array[:, last_column_number-1:last_column_number]), axis = 1)\n",
    "    \n",
    "    # convert the dataset to a dataframe\n",
    "    # Here the column names are the original column names and the one hot encoded column names\n",
    "    # and the values are the values of the dataset array\n",
    "    dataset = pd.DataFrame(data=dataset_array, columns = new_column_names)\n",
    "    \n",
    "    # restore the original data types of the columns\n",
    "    for column_name in dataset.columns:\n",
    "        if column_name in original_data_types:\n",
    "            dataset[column_name] = dataset[column_name].astype(original_data_types[column_name])\n",
    "        else:\n",
    "            dataset[column_name] = dataset[column_name].astype('int64')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = convert_categorical(dataset, categorical_col_names)\n",
    "\n",
    "print(dataset.dtypes)\n",
    "# print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset into dependent and independent variables\n",
    "\n",
    "We will divide the dataset into dependent and independent variables. \n",
    "\n",
    "The dependent variable is the variable that we want to predict.The independent variables are the variables that we will use to predict the dependent variable. In this case, \n",
    "\n",
    "- It is the `Churn` column. \n",
    "- All the other columns except the `Churn` column and `Customer ID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to divide the dataset into x and y\n",
    "def divide_dataset(dataset):\n",
    "    # divide the dataset into x and y\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    print(dataset_columns_length)\n",
    "\n",
    "    x = dataset.iloc[:, 0:(dataset_columns_length-1)].values\n",
    "    y = dataset.iloc[:, (dataset_columns_length-1)].values\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = divide_dataset(dataset)\n",
    "# What is the shape of x and y?\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# What is the value of the 1st row of x?\n",
    "print(x[0])\n",
    "\n",
    "# What is the value of the 1st row of y?\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the dataset into training and testing sets\n",
    "\n",
    "We will split the dataset into training and testing sets. We will use the `train_test_split` method from the `sklearn.model_selection` module to split the dataset into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset into training and testing set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# function to split the dataset into training and testing set\n",
    "def split_dataset(x, y):\n",
    "    # split the dataset into training and testing set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_dataset(x, y)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infomation gain of the features\n",
    "\n",
    "We will calculate the information gain of the features. We will use the `mutual_info_classif` method from the `sklearn.feature_selection` module to calculate the information gain of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "def entropy(y):\n",
    "    hist = np.bincount(y)\n",
    "    ps = hist / len(y)\n",
    "    return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "def information_gain(X, y, feature_index):\n",
    "    # Calculate the entropy of the target\n",
    "    parent_entropy = entropy(y)\n",
    "\n",
    "    # Find the median of the feature\n",
    "    median = np.median(X[:, feature_index])\n",
    "\n",
    "    # Split the dataset based on the median\n",
    "    left_indices = np.where(X[:, feature_index] <= median)\n",
    "    right_indices = np.where(X[:, feature_index] > median)\n",
    "\n",
    "    # Calculate the entropy of each subset\n",
    "    left_entropy = entropy(y[left_indices])\n",
    "    right_entropy = entropy(y[right_indices])\n",
    "\n",
    "    # Calculate the number of instances in each subset\n",
    "    num_left = len(left_indices[0]) / len(y)\n",
    "    num_right = len(right_indices[0]) / len(y)\n",
    "\n",
    "    # Calculate the weighted sum of the subset entropies\n",
    "    child_entropy = num_left * left_entropy + num_right * right_entropy\n",
    "\n",
    "    # Calculate and return the information gain\n",
    "    return parent_entropy - child_entropy\n",
    "\n",
    "# function to find the mutual information of each feature\n",
    "def calculate_information_gain(X, y):\n",
    "    # Calculate the mutual information between each feature and the target\n",
    "    mutual_info = mutual_info_classif(X, y)\n",
    "    \n",
    "    # Normalize the mutual information values to get information gain\n",
    "    information_gain = mutual_info / np.sum(mutual_info)\n",
    "    \n",
    "    return information_gain\n",
    "\n",
    "information_gain = calculate_information_gain(x_train, y_train)\n",
    "# information_gain_values = np.array([information_gain(x_train, y_train, i) for i in range(x_train.shape[1])])\n",
    "\n",
    "# Create a bar plot of the information gain values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(information_gain)), information_gain)\n",
    "plt.title('Information Gain of Each Feature')\n",
    "plt.xlabel('Feature Index')\n",
    "plt.ylabel('Information Gain')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Linear Regression class\n",
    "\n",
    "We will create a custom linear regression class with the following tweaks:\n",
    "\n",
    "1. Use information gain to evaluate attribute importance in order to use a subset of features.\n",
    "1. Control the number of features using an external parameter\n",
    "1. Early terminate Gradient Descent if error in the training set becomes < 0.5. \n",
    "    - Parameterize your function to take the threshold as an input. [if error < threshold, break]\n",
    "1. Use sigmoid function. We need to calculate the gradient and derive the update rule for the weights accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class CustomLogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, num_iterations=100000, early_stopping_threshold=0.5, num_features=None, verbose=False, fit_intercept=True):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_iterations = num_iterations\n",
    "        self.early_stopping_threshold = early_stopping_threshold\n",
    "        self.num_features = num_features\n",
    "        self.verbose = verbose\n",
    "        self.fit_intercept = fit_intercept\n",
    "        self.top_features_indices = None\n",
    "        \n",
    "        \n",
    "    def entropy(self, y):\n",
    "        hist = np.bincount(y)\n",
    "        ps = hist / len(y)\n",
    "        return -np.sum([p * np.log2(p) for p in ps if p > 0])\n",
    "\n",
    "    def information_gain(self, X, y, feature_index):\n",
    "        parent_entropy = self.entropy(y)\n",
    "        median = np.median(X[:, feature_index])\n",
    "        left_indices = np.where(X[:, feature_index] <= median)\n",
    "        right_indices = np.where(X[:, feature_index] > median)\n",
    "        left_entropy = self.entropy(y[left_indices])\n",
    "        right_entropy = self.entropy(y[right_indices])\n",
    "        num_left = len(left_indices[0]) / len(y)\n",
    "        num_right = len(right_indices[0]) / len(y)\n",
    "        child_entropy = num_left * left_entropy + num_right * right_entropy\n",
    "        return parent_entropy - child_entropy\n",
    "\n",
    "    def __add_intercept(self, X):\n",
    "        intercept = np.ones((X.shape[0], 1))\n",
    "        return np.concatenate((intercept, X), axis=1)\n",
    "\n",
    "    def __sigmoid(self, z):\n",
    "        # Limit the values in z to avoid overflow\n",
    "        z = np.clip(z, -250, 250)\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def __loss(self, h, y):\n",
    "        # Add small constants to avoid taking the log of zero\n",
    "        return (-y * np.log(h + 1e-10) - (1 - y) * np.log(1 - h + 1e-10)).mean()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "         # Select the top features based on information gain\n",
    "        if self.num_features is not None and self.num_features < X.shape[1] - 1 and self.num_features > 0:\n",
    "            information_gain_values = np.array([self.information_gain(X, y, i) for i in range(X.shape[1])])\n",
    "            self.top_features_indices = np.argsort(-information_gain_values)[:self.num_features]\n",
    "            X = X[:, self.top_features_indices]\n",
    "\n",
    "        # weights initialization\n",
    "        self.theta = np.zeros(X.shape[1])\n",
    "\n",
    "        for i in range(self.num_iterations):\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            gradient = np.dot(X.T, (h - y)) / y.size\n",
    "            self.theta -= self.learning_rate * gradient\n",
    "\n",
    "            z = np.dot(X, self.theta)\n",
    "            h = self.__sigmoid(z)\n",
    "            loss = self.__loss(h, y)\n",
    "\n",
    "            if self.verbose and i % 10000 == 0:\n",
    "                print(f'loss: {loss} \\t')\n",
    "\n",
    "            if loss < self.early_stopping_threshold:\n",
    "                break\n",
    "\n",
    "    def predict_prob(self, X):\n",
    "        if self.fit_intercept:\n",
    "            X = self.__add_intercept(X)\n",
    "\n",
    "        if self.num_features is not None and self.num_features < X.shape[1] - 1 and self.num_features > 0:\n",
    "            if self.verbose:\n",
    "                print(f'top features indices: {self.top_features_indices}')\n",
    "            X = X[:, self.top_features_indices]\n",
    "\n",
    "        return self.__sigmoid(np.dot(X, self.theta))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_prob(X) >= threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomLogisticRegression(learning_rate=0.01, num_iterations=100000, early_stopping_threshold=0.4, verbose=True)\n",
    "\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
