{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card data\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "\n",
    "This dataset can be found in the following [link](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
    "\n",
    "This dataset has information about the credit card fraud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Time      float64\n",
      "V1        float64\n",
      "V2        float64\n",
      "V3        float64\n",
      "V4        float64\n",
      "V5        float64\n",
      "V6        float64\n",
      "V7        float64\n",
      "V8        float64\n",
      "V9        float64\n",
      "V10       float64\n",
      "V11       float64\n",
      "V12       float64\n",
      "V13       float64\n",
      "V14       float64\n",
      "V15       float64\n",
      "V16       float64\n",
      "V17       float64\n",
      "V18       float64\n",
      "V19       float64\n",
      "V20       float64\n",
      "V21       float64\n",
      "V22       float64\n",
      "V23       float64\n",
      "V24       float64\n",
      "V25       float64\n",
      "V26       float64\n",
      "V27       float64\n",
      "V28       float64\n",
      "Amount    float64\n",
      "Class       int64\n",
      "dtype: object\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "\n",
      "[2 rows x 31 columns]\n",
      "            Time        V1        V2        V3        V4        V5        V6  \\\n",
      "284805  172788.0 -0.240440  0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
      "284806  172792.0 -0.533413 -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
      "\n",
      "              V7        V8        V9  ...       V21       V22       V23  \\\n",
      "284805 -0.686180  0.679145  0.392087  ...  0.265245  0.800049 -0.163298   \n",
      "284806  1.577006 -0.414650  0.486180  ...  0.261057  0.643078  0.376777   \n",
      "\n",
      "             V24       V25       V26       V27       V28  Amount  Class  \n",
      "284805  0.123205 -0.569159  0.546668  0.108821  0.104533    10.0      0  \n",
      "284806  0.008797 -0.473649 -0.818267 -0.002415  0.013649   217.0      0  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Credit card classification\n",
    "\n",
    "# import statement\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    # load dataset\n",
    "    dataset = pd.read_csv(file_name)\n",
    "    \n",
    "    # replace the cells with single and extra spaces with a Nan value\n",
    "    dataset.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "    \n",
    "    # convert the dataset to dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_data('creditcard.csv')\n",
    "\n",
    "# print the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# print the datatypes of the dataset\n",
    "print(dataset.dtypes)\n",
    "\n",
    "# head and tail of the dataset\n",
    "print(dataset.head(2))\n",
    "print(dataset.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the missing values\n",
    "\n",
    "we will check for the missing values in the dataset.\n",
    "\n",
    "- If there are any missing values, we will replace them with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()  # There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accounting for the bias of the dataset\n",
    "\n",
    "We will check for the bias in the dataset and try to remove it.\n",
    "\n",
    "- In this dataset there is a bias of 99.8% of non-fraudulent transactions and 0.2% of fraudulent transactions.\n",
    "- We will try to remove this bias by using the following methods:\n",
    "    - Keep all the fraudulent transactions and randomly select the roughly twice of the number of fraudulent transactions from the non-fraudulent transactions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class\n",
      "0    2843\n",
      "1     492\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check for the bias in the dataset\n",
    "dataset['Class'].value_counts()  # The dataset is biased\n",
    "\n",
    "# create a new dataframe with all the rows with class 1 and \n",
    "# 1% of the rows with class 0\n",
    "df1 = dataset[dataset['Class'] == 1]\n",
    "df0 = dataset[dataset['Class'] == 0].sample(frac = 0.01)\n",
    "\n",
    "# concatenate the two dataframes\n",
    "dataset = pd.concat([df1, df0], axis = 0)\n",
    "\n",
    "counts = dataset['Class'].value_counts()\n",
    "\n",
    "print(counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data \n",
    "\n",
    "We will scale the data using the `minmax_scaler` from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Time        V1        V2        V3        V4        V5        V6  \\\n",
      "541  0.002145  0.874410  0.607334  0.845728  0.495592  0.650089  0.252074   \n",
      "623  0.002528  0.854614  0.507549  0.923101  0.389371  0.706774  0.270385   \n",
      "\n",
      "          V7        V8        V9  ...       V21       V22       V23       V24  \\\n",
      "541  0.61194  0.695084  0.498968  ...  0.466293  0.513187  0.609497  0.484688   \n",
      "623  0.65465  0.671179  0.615903  ...  0.469182  0.540466  0.669223  0.377722   \n",
      "\n",
      "          V25       V26       V27       V28    Amount  Class  \n",
      "541  0.603535  0.361456  0.694520  0.183784  0.000000      1  \n",
      "623  0.632958  0.284087  0.647086  0.193047  0.124519      1  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "def scale_data(dataset):\n",
    "    # get the column names of each column without the last column\n",
    "    columns = dataset.columns[:-1]\n",
    "    # for each column, get the max and min values\n",
    "    for column in columns:\n",
    "        max_value = dataset[column].max()\n",
    "        min_value = dataset[column].min()\n",
    "        dataset[column] = (dataset[column] - min_value)/(max_value - min_value)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "dataset = scale_data(dataset)\n",
    "\n",
    "# print the head of the dataset\n",
    "print(dataset.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into dependent and independent variables\n",
    "\n",
    "We will divide the data into dependent and independent variables\n",
    "\n",
    "- The dependent variable is `Class`\n",
    "- The independent variables are the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "(3335, 30)\n",
      "(3335,)\n",
      "[0.0021454  0.87441016 0.60733428 0.8457275  0.49559176 0.65008857\n",
      " 0.25207422 0.61194017 0.69508431 0.49896826 0.59968213 0.42903751\n",
      " 0.73290202 0.3979715  0.62048809 0.54682354 0.67822092 0.70003809\n",
      " 0.71351337 0.46001643 0.69868558 0.46629259 0.51318723 0.60949692\n",
      " 0.484688   0.60353476 0.36145596 0.69452029 0.18378381 0.        ]\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# function to divide the dataset into dependent and independent variables\n",
    "def divide_dataset(dataset):\n",
    "    # divide the dataset into x and y\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    print(dataset_columns_length)\n",
    "\n",
    "    x = dataset.iloc[:, 0:(dataset_columns_length-1)].values\n",
    "    y = dataset.iloc[:, (dataset_columns_length-1)].values\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = divide_dataset(dataset)\n",
    "\n",
    "# print the shape of x and y\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets\n",
    "\n",
    "We will split the data into train and test sets using the `train_test_split` from `sklearn.model_selection`\n",
    "\n",
    "- The train set will have 70% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2334, 30)\n",
      "(1001, 30)\n",
      "(2334,)\n",
      "(1001,)\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# funciton to split the dataset into training and testing set\n",
    "def split_dataset(x, y):\n",
    "    # split the dataset into training and testing set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_dataset(x, y)\n",
    "\n",
    "# print the shape of x_train, x_test, y_train, y_test\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "- import the custom model from `custom_logistic_regression.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6817225298714803 \t\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[855   0]\n",
      " [ 68  78]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      1.00      0.96       855\n",
      "           1       1.00      0.53      0.70       146\n",
      "\n",
      "    accuracy                           0.93      1001\n",
      "   macro avg       0.96      0.77      0.83      1001\n",
      "weighted avg       0.94      0.93      0.92      1001\n",
      "\n",
      "0.932067932067932\n"
     ]
    }
   ],
   "source": [
    "from custom_logistic_regression import CustomLogisticRegression\n",
    "\n",
    "# create an object of CustomLogisticRegression\n",
    "classifier = CustomLogisticRegression(learning_rate=0.01, early_stopping_threshold=0.01, verbose=True, num_features=20)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict the values\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# print the accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
