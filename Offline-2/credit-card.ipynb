{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit card data\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "\n",
    "This dataset can be found in the following [link](https://www.kaggle.com/datasets/mlg-ulb/creditcardfraud)\n",
    "\n",
    "This dataset has information about the credit card fraud\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "Time      float64\n",
      "V1        float64\n",
      "V2        float64\n",
      "V3        float64\n",
      "V4        float64\n",
      "V5        float64\n",
      "V6        float64\n",
      "V7        float64\n",
      "V8        float64\n",
      "V9        float64\n",
      "V10       float64\n",
      "V11       float64\n",
      "V12       float64\n",
      "V13       float64\n",
      "V14       float64\n",
      "V15       float64\n",
      "V16       float64\n",
      "V17       float64\n",
      "V18       float64\n",
      "V19       float64\n",
      "V20       float64\n",
      "V21       float64\n",
      "V22       float64\n",
      "V23       float64\n",
      "V24       float64\n",
      "V25       float64\n",
      "V26       float64\n",
      "V27       float64\n",
      "V28       float64\n",
      "Amount    float64\n",
      "Class       int64\n",
      "dtype: object\n",
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0 -1.359807 -0.072781  2.536347  1.378155 -0.338321  0.462388  0.239599   \n",
      "1   0.0  1.191857  0.266151  0.166480  0.448154  0.060018 -0.082361 -0.078803   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.098698  0.363787  ... -0.018307  0.277838 -0.110474  0.066928  0.128539   \n",
      "1  0.085102 -0.255425  ... -0.225775 -0.638672  0.101288 -0.339846  0.167170   \n",
      "\n",
      "        V26       V27       V28  Amount  Class  \n",
      "0 -0.189115  0.133558 -0.021053  149.62      0  \n",
      "1  0.125895 -0.008983  0.014724    2.69      0  \n",
      "\n",
      "[2 rows x 31 columns]\n",
      "            Time        V1        V2        V3        V4        V5        V6  \\\n",
      "284805  172788.0 -0.240440  0.530483  0.702510  0.689799 -0.377961  0.623708   \n",
      "284806  172792.0 -0.533413 -0.189733  0.703337 -0.506271 -0.012546 -0.649617   \n",
      "\n",
      "              V7        V8        V9  ...       V21       V22       V23  \\\n",
      "284805 -0.686180  0.679145  0.392087  ...  0.265245  0.800049 -0.163298   \n",
      "284806  1.577006 -0.414650  0.486180  ...  0.261057  0.643078  0.376777   \n",
      "\n",
      "             V24       V25       V26       V27       V28  Amount  Class  \n",
      "284805  0.123205 -0.569159  0.546668  0.108821  0.104533    10.0      0  \n",
      "284806  0.008797 -0.473649 -0.818267 -0.002415  0.013649   217.0      0  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Credit card classification\n",
    "\n",
    "# import statement\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "\n",
    "def load_data(file_name):\n",
    "    # load dataset\n",
    "    dataset = pd.read_csv(file_name)\n",
    "    \n",
    "    # replace the cells with single and extra spaces with a Nan value\n",
    "    dataset.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "    \n",
    "    # convert the dataset to dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_data('creditcard.csv')\n",
    "\n",
    "# print the shape of the dataset\n",
    "print(dataset.shape)\n",
    "\n",
    "# print the datatypes of the dataset\n",
    "print(dataset.dtypes)\n",
    "\n",
    "# head and tail of the dataset\n",
    "print(dataset.head(2))\n",
    "print(dataset.tail(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the missing values\n",
    "\n",
    "we will check for the missing values in the dataset.\n",
    "\n",
    "- If there are any missing values, we will replace them with the mean of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Time      0\n",
       "V1        0\n",
       "V2        0\n",
       "V3        0\n",
       "V4        0\n",
       "V5        0\n",
       "V6        0\n",
       "V7        0\n",
       "V8        0\n",
       "V9        0\n",
       "V10       0\n",
       "V11       0\n",
       "V12       0\n",
       "V13       0\n",
       "V14       0\n",
       "V15       0\n",
       "V16       0\n",
       "V17       0\n",
       "V18       0\n",
       "V19       0\n",
       "V20       0\n",
       "V21       0\n",
       "V22       0\n",
       "V23       0\n",
       "V24       0\n",
       "V25       0\n",
       "V26       0\n",
       "V27       0\n",
       "V28       0\n",
       "Amount    0\n",
       "Class     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()  # There are no missing values in the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data \n",
    "\n",
    "We will scale the data using the `minmax_scaler` from `sklearn.preprocessing`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Time        V1        V2        V3        V4        V5        V6        V7  \\\n",
      "0   0.0  0.935192  0.766490  0.881365  0.313023  0.763439  0.267669  0.266815   \n",
      "1   0.0  0.978542  0.770067  0.840298  0.271796  0.766120  0.262192  0.264875   \n",
      "\n",
      "         V8        V9  ...       V21       V22       V23       V24       V25  \\\n",
      "0  0.786444  0.475312  ...  0.561184  0.522992  0.663793  0.391253  0.585122   \n",
      "1  0.786298  0.453981  ...  0.557840  0.480237  0.666938  0.336440  0.587290   \n",
      "\n",
      "        V26       V27       V28    Amount  Class  \n",
      "0  0.394557  0.418976  0.312697  0.005824      0  \n",
      "1  0.446013  0.416345  0.313423  0.000105      0  \n",
      "\n",
      "[2 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "def scale_data(dataset):\n",
    "    # get the column names of each column without the last column\n",
    "    columns = dataset.columns[:-1]\n",
    "    # for each column, get the max and min values\n",
    "    for column in columns:\n",
    "        max_value = dataset[column].max()\n",
    "        min_value = dataset[column].min()\n",
    "        dataset[column] = (dataset[column] - min_value)/(max_value - min_value)\n",
    "        \n",
    "    return dataset\n",
    "\n",
    "dataset = scale_data(dataset)\n",
    "\n",
    "# print the head of the dataset\n",
    "print(dataset.head(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into dependent and independent variables\n",
    "\n",
    "We will divide the data into dependent and independent variables\n",
    "\n",
    "- The dependent variable is `Class`\n",
    "- The independent variables are the rest of the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "(284807, 30)\n",
      "(284807,)\n",
      "[0.         0.93519234 0.76649042 0.8813649  0.31302266 0.76343873\n",
      " 0.26766864 0.26681518 0.7864442  0.47531173 0.51060048 0.25248432\n",
      " 0.68090763 0.3715906  0.63559053 0.4460837  0.43439239 0.73717255\n",
      " 0.65506586 0.59486323 0.58294223 0.56118439 0.52299212 0.66379298\n",
      " 0.39125268 0.58512179 0.39455679 0.41897614 0.31269663 0.00582379]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# function to divide the dataset into dependent and independent variables\n",
    "def divide_dataset(dataset):\n",
    "    # divide the dataset into x and y\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    print(dataset_columns_length)\n",
    "\n",
    "    x = dataset.iloc[:, 0:(dataset_columns_length-1)].values\n",
    "    y = dataset.iloc[:, (dataset_columns_length-1)].values\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = divide_dataset(dataset)\n",
    "\n",
    "# print the shape of x and y\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "print(x[0])\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into train and test sets\n",
    "\n",
    "We will split the data into train and test sets using the `train_test_split` from `sklearn.model_selection`\n",
    "\n",
    "- The train set will have 70% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(199364, 30)\n",
      "(85443, 30)\n",
      "(199364,)\n",
      "(85443,)\n",
      "float64\n",
      "float64\n",
      "int64\n",
      "int64\n"
     ]
    }
   ],
   "source": [
    "# import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# funciton to split the dataset into training and testing set\n",
    "def split_dataset(x, y):\n",
    "    # split the dataset into training and testing set\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_dataset(x, y)\n",
    "\n",
    "# print the shape of x_train, x_test, y_train, y_test\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print(x_train.dtype)\n",
    "print(x_test.dtype)\n",
    "print(y_train.dtype)\n",
    "print(y_test.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "- import the custom model from `custom_logistic_regression.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6749623896181135 \t\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/akib/Repos/course_work/CSE-472/Offline-2/credit-card.ipynb Cell 12\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akib/Repos/course_work/CSE-472/Offline-2/credit-card.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# create an object of CustomLogisticRegression\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akib/Repos/course_work/CSE-472/Offline-2/credit-card.ipynb#X14sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m classifier \u001b[39m=\u001b[39m CustomLogisticRegression(learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, num_iterations\u001b[39m=\u001b[39m\u001b[39m20000\u001b[39m, early_stopping_threshold\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, num_features\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/akib/Repos/course_work/CSE-472/Offline-2/credit-card.ipynb#X14sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m classifier\u001b[39m.\u001b[39;49mfit(x_train, y_train)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akib/Repos/course_work/CSE-472/Offline-2/credit-card.ipynb#X14sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m# predict the values\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/akib/Repos/course_work/CSE-472/Offline-2/credit-card.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m y_pred \u001b[39m=\u001b[39m classifier\u001b[39m.\u001b[39mpredict(x_test)\n",
      "File \u001b[0;32m~/Repos/course_work/CSE-472/Offline-2/custom_logistic_regression.py:65\u001b[0m, in \u001b[0;36mCustomLogisticRegression.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     63\u001b[0m z \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(X, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtheta)\n\u001b[1;32m     64\u001b[0m h \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__sigmoid(z)\n\u001b[0;32m---> 65\u001b[0m loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__loss(h, y)\n\u001b[1;32m     67\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39mand\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m10000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     68\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss: \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Repos/course_work/CSE-472/Offline-2/custom_logistic_regression.py:42\u001b[0m, in \u001b[0;36mCustomLogisticRegression.__loss\u001b[0;34m(self, h, y)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__loss\u001b[39m(\u001b[39mself\u001b[39m, h, y):\n\u001b[1;32m     41\u001b[0m     \u001b[39m# Add small constants to avoid taking the log of zero\u001b[39;00m\n\u001b[0;32m---> 42\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39m-\u001b[39my \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39;49mlog(h \u001b[39m+\u001b[39;49m \u001b[39m1e-10\u001b[39;49m) \u001b[39m-\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m y) \u001b[39m*\u001b[39m np\u001b[39m.\u001b[39mlog(\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m h \u001b[39m+\u001b[39m \u001b[39m1e-10\u001b[39m))\u001b[39m.\u001b[39mmean()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from custom_logistic_regression import CustomLogisticRegression\n",
    "\n",
    "# create an object of CustomLogisticRegression\n",
    "classifier = CustomLogisticRegression(learning_rate=0.01, num_iterations=20000, early_stopping_threshold=0.01, verbose=True, num_features=20)\n",
    "\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict the values\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# print the accuracy\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
