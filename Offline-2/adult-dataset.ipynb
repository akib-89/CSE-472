{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset about adults\n",
    "\n",
    "### About the dataset\n",
    "\n",
    "This dataset contains information about adults. The goal is to predict whether a person earns over 50K a year or not.\n",
    "\n",
    "### Attributes\n",
    "\n",
    "- `age`: continuous.\n",
    "- `workclass`: Private, Self-emp-not-inc, Self-emp-inc, Federal-gov, Local-gov, State-gov, Without-pay, Never-worked.\n",
    "- `fnlwgt`: continuous.\n",
    "- `education`: Bachelors, Some-college, 11th, HS-grad, Prof-school, Assoc-acdm, Assoc-voc, 9th, 7th-8th, 12th, Masters, 1st-4th, 10th, Doctorate, 5th-6th, Preschool. (we will not use this attribute because it is redundant with `education-num`)\n",
    "- `education-num`: continuous.\n",
    "- `marital-status`: Married-civ-spouse, Divorced, Never-married, Separated, Widowed, Married-spouse-absent, Married-AF-spouse.\n",
    "- `occupation`: Tech-support, Craft-repair, Other-service, Sales, Exec-managerial, Prof-specialty, Handlers-cleaners, Machine-op-inspct, Adm-clerical, Farming-fishing, Transport-moving, Priv-house-serv, Protective-serv, Armed-Forces.\n",
    "- `relationship`: Wife, Own-child, Husband, Not-in-family, Other-relative, Unmarried.\n",
    "- `race`: White, Asian-Pac-Islander, Amer-Indian-Eskimo, Other, Black.\n",
    "- `sex`: Female, Male\n",
    "- `capital-gain`: continuous.\n",
    "- `capital-loss`: continuous.\n",
    "- `hours-per-week`: continuous.\n",
    "- `native-country`: United-States, Cambodia, England, Puerto-Rico, Canada, Germany, Outlying-US(Guam-USVI-etc), India, Japan, Greece, South, China, Cuba, Iran, Honduras, Philippines, Italy, Poland, Jamaica, Vietnam, Mexico, Portugal, Ireland, France, Dominican-Republic, Laos, Ecuador, Taiwan, Haiti, Columbia, Hungary, Guatemala, Nicaragua, Scotland, Thailand, Yugoslavia, El-Salvador, Trinadad&Tobago, Peru, Hong, Holand-Netherlands.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                                40\n",
      "workclass                     Private\n",
      "fnlwgt                         121772\n",
      "education-num                      11\n",
      "marital-status     Married-civ-spouse\n",
      "occupation               Craft-repair\n",
      "relationship                  Husband\n",
      "race               Asian-Pac-Islander\n",
      "sex                              Male\n",
      "capital-gain                        0\n",
      "capital-loss                        0\n",
      "hours-per-week                     40\n",
      "native-country                    NaN\n",
      "income                           >50K\n",
      "Name: 14, dtype: object\n",
      "(32561, 14)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 32561 entries, 0 to 32560\n",
      "Data columns (total 14 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   age             32561 non-null  int64 \n",
      " 1   workclass       30725 non-null  object\n",
      " 2   fnlwgt          32561 non-null  int64 \n",
      " 3   education-num   32561 non-null  int64 \n",
      " 4   marital-status  32561 non-null  object\n",
      " 5   occupation      30718 non-null  object\n",
      " 6   relationship    32561 non-null  object\n",
      " 7   race            32561 non-null  object\n",
      " 8   sex             32561 non-null  object\n",
      " 9   capital-gain    32561 non-null  int64 \n",
      " 10  capital-loss    32561 non-null  int64 \n",
      " 11  hours-per-week  32561 non-null  int64 \n",
      " 12  native-country  31978 non-null  object\n",
      " 13  income          32561 non-null  object\n",
      "dtypes: int64(6), object(8)\n",
      "memory usage: 3.5+ MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "\n",
    "def load_data(file_name, name_of_columns):\n",
    "    \"\"\"\n",
    "    Load data from a file into a pandas dataframe\n",
    "    :param file_name: name of file to load\n",
    "    :return: pandas dataframe\n",
    "    \"\"\"\n",
    "    dataset = pd.read_csv(file_name, header=None, names=name_of_columns)\n",
    "    \n",
    "    \n",
    "    # replace the cells with ' ?' with NaN\n",
    "    dataset = dataset.replace(' ?', np.NaN)\n",
    "    \n",
    "    # replce the cells with single or extra spaces with NaN\n",
    "    dataset.replace(r'^\\s*$', np.NaN, regex=True, inplace=True)\n",
    "    \n",
    "    # drop the education column as it is redundant\n",
    "    dataset.drop('education', axis=1, inplace=True)\n",
    "    \n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    return dataset\n",
    "\n",
    "name_of_columns = ['age', 'workclass', 'fnlwgt', 'education', 'education-num', 'marital-status', 'occupation',\n",
    "                   'relationship', 'race', 'sex', 'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income']\n",
    "\n",
    "dataset = load_data('adult.data', name_of_columns)\n",
    "\n",
    "test_dataset = load_data('adult.test', name_of_columns)\n",
    "\n",
    "# print the 15th row of the dataset\n",
    "print(dataset.iloc[14])\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "print(dataset.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking the dataset for missing values\n",
    "\n",
    "- `?` is used to represent missing values in this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age                 0\n",
       "workclass         963\n",
       "fnlwgt              0\n",
       "education-num       0\n",
       "marital-status      0\n",
       "occupation        966\n",
       "relationship        0\n",
       "race                0\n",
       "sex                 0\n",
       "capital-gain        0\n",
       "capital-loss        0\n",
       "hours-per-week      0\n",
       "native-country    274\n",
       "income              0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.isnull().sum()\n",
    "test_dataset.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 workclass 1836\n",
      "5 occupation 1843\n",
      "12 native-country 583\n",
      "[1, 5, 12]\n",
      "1 workclass 963\n",
      "5 occupation 966\n",
      "12 native-country 274\n",
      "[1, 5, 12]\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# function to find the missing columns\n",
    "def find_missing_columns(dataset):\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    # Store the column number of the columns with missing values in a list called missing_cols\n",
    "    missing_cols = [i for i in range(dataset_columns_length) if dataset.iloc[:, i].isnull().any()]\n",
    "    \n",
    "    # Print columns index and names with missing values and the number of missing values\n",
    "    for i in missing_cols:\n",
    "        print(i, dataset.columns[i], dataset.iloc[:, i].isnull().sum())\n",
    "            \n",
    "    return missing_cols\n",
    "\n",
    "def impute_missing_values(dataset, missing_columns):\n",
    "    for column in missing_columns:\n",
    "        column_name = dataset.columns[column]\n",
    "        if dataset[column_name].dtype == 'object' or dataset[column_name].dtype == 'int64':\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            dataset[column_name] = imputer.fit_transform(dataset[column_name].values.reshape(-1, 1)).ravel()\n",
    "        elif dataset[column_name].dtype == 'float64':\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            dataset[column_name] = imputer.fit_transform(dataset[column_name].values.reshape(-1, 1)).ravel()\n",
    "    return dataset\n",
    "\n",
    "# find the missing columns in the dataset\n",
    "missing_cols = find_missing_columns(dataset)\n",
    "\n",
    "print(missing_cols)\n",
    "\n",
    "dataset = impute_missing_values(dataset, missing_cols)\n",
    "\n",
    "missing_cols_test = find_missing_columns(test_dataset)\n",
    "\n",
    "print(missing_cols_test)\n",
    "\n",
    "test_dataset = impute_missing_values(test_dataset, missing_cols_test)\n",
    "\n",
    "print(dataset.isnull().sum())       # verify\n",
    "print(test_dataset.isnull().sum())  # verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the label to a binary value\n",
    "\n",
    "- `<=50K` is converted to `0`\n",
    "- `>50K` is converted to `1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income             int64\n",
      "dtype: object\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n",
      "age                int64\n",
      "workclass         object\n",
      "fnlwgt             int64\n",
      "education-num      int64\n",
      "marital-status    object\n",
      "occupation        object\n",
      "relationship      object\n",
      "race              object\n",
      "sex               object\n",
      "capital-gain       int64\n",
      "capital-loss       int64\n",
      "hours-per-week     int64\n",
      "native-country    object\n",
      "income             int64\n",
      "dtype: object\n",
      "age               0\n",
      "workclass         0\n",
      "fnlwgt            0\n",
      "education-num     0\n",
      "marital-status    0\n",
      "occupation        0\n",
      "relationship      0\n",
      "race              0\n",
      "sex               0\n",
      "capital-gain      0\n",
      "capital-loss      0\n",
      "hours-per-week    0\n",
      "native-country    0\n",
      "income            0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# convert the label column to binary values\n",
    "\n",
    "def convert_label_to_binary(dataset, label_column_name, options):\n",
    "    dataset[label_column_name] = dataset[label_column_name].map(options)\n",
    "    return dataset\n",
    "\n",
    "dataset = convert_label_to_binary(dataset, 'income', {' <=50K': 0, ' >50K': 1})\n",
    "test_dataset = convert_label_to_binary(test_dataset, 'income', {' <=50K.': 0, ' >50K.': 1})\n",
    "\n",
    "# print the 15th row of the dataset\n",
    "# print(dataset.iloc[14])\n",
    "\n",
    "print(dataset.dtypes)\n",
    "print(dataset.isnull().sum())\n",
    "\n",
    "print(test_dataset.dtypes)\n",
    "print(test_dataset.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the categorical attributes to numerical attributes\n",
    "\n",
    "For converting the categorical values to numerical values, we use the `OneHotEncoder` class from `sklearn.preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss',\n",
      "       'hours-per-week', 'workclass_ Federal-gov', 'workclass_ Local-gov',\n",
      "       'workclass_ Never-worked', 'workclass_ Private',\n",
      "       'workclass_ Self-emp-inc', 'workclass_ Self-emp-not-inc',\n",
      "       'workclass_ State-gov', 'workclass_ Without-pay',\n",
      "       'marital-status_ Divorced', 'marital-status_ Married-AF-spouse',\n",
      "       'marital-status_ Married-civ-spouse',\n",
      "       'marital-status_ Married-spouse-absent',\n",
      "       'marital-status_ Never-married', 'marital-status_ Separated',\n",
      "       'marital-status_ Widowed', 'relationship_ Husband',\n",
      "       'relationship_ Not-in-family', 'relationship_ Other-relative',\n",
      "       'relationship_ Own-child', 'relationship_ Unmarried',\n",
      "       'relationship_ Wife', 'occupation_ Adm-clerical',\n",
      "       'occupation_ Armed-Forces', 'occupation_ Craft-repair',\n",
      "       'occupation_ Exec-managerial', 'occupation_ Farming-fishing',\n",
      "       'occupation_ Handlers-cleaners', 'occupation_ Machine-op-inspct',\n",
      "       'occupation_ Other-service', 'occupation_ Priv-house-serv',\n",
      "       'occupation_ Prof-specialty', 'occupation_ Protective-serv',\n",
      "       'occupation_ Sales', 'occupation_ Tech-support',\n",
      "       'occupation_ Transport-moving', 'race_ Amer-Indian-Eskimo',\n",
      "       'race_ Asian-Pac-Islander', 'race_ Black', 'race_ Other', 'race_ White',\n",
      "       'sex_ Female', 'sex_ Male', 'native-country_ Cambodia',\n",
      "       'native-country_ Canada', 'native-country_ China',\n",
      "       'native-country_ Columbia', 'native-country_ Cuba',\n",
      "       'native-country_ Dominican-Republic', 'native-country_ Ecuador',\n",
      "       'native-country_ El-Salvador', 'native-country_ England',\n",
      "       'native-country_ France', 'native-country_ Germany',\n",
      "       'native-country_ Greece', 'native-country_ Guatemala',\n",
      "       'native-country_ Haiti', 'native-country_ Holand-Netherlands',\n",
      "       'native-country_ Honduras', 'native-country_ Hong',\n",
      "       'native-country_ Hungary', 'native-country_ India',\n",
      "       'native-country_ Iran', 'native-country_ Ireland',\n",
      "       'native-country_ Italy', 'native-country_ Jamaica',\n",
      "       'native-country_ Japan', 'native-country_ Laos',\n",
      "       'native-country_ Mexico', 'native-country_ Nicaragua',\n",
      "       'native-country_ Outlying-US(Guam-USVI-etc)', 'native-country_ Peru',\n",
      "       'native-country_ Philippines', 'native-country_ Poland',\n",
      "       'native-country_ Portugal', 'native-country_ Puerto-Rico',\n",
      "       'native-country_ Scotland', 'native-country_ South',\n",
      "       'native-country_ Taiwan', 'native-country_ Thailand',\n",
      "       'native-country_ Trinadad&Tobago', 'native-country_ United-States',\n",
      "       'native-country_ Vietnam', 'native-country_ Yugoslavia', 'income'],\n",
      "      dtype='object')\n",
      "(32561, 90)\n",
      "(16281, 90)\n",
      "age                                int64\n",
      "fnlwgt                             int64\n",
      "education-num                      int64\n",
      "capital-gain                       int64\n",
      "capital-loss                       int64\n",
      "                                   ...  \n",
      "native-country_ Trinadad&Tobago    int64\n",
      "native-country_ United-States      int64\n",
      "native-country_ Vietnam            int64\n",
      "native-country_ Yugoslavia         int64\n",
      "income                             int64\n",
      "Length: 90, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# function to convert the categorical values to numerical values using one hot encoding\n",
    "def convert_categorical(dataset, column_names_to_convert):\n",
    "    column_index = []\n",
    "    \n",
    "    # find the index of the categorical columns\n",
    "    for column_name in column_names_to_convert:\n",
    "        column_index.append(dataset.columns.get_loc(column_name))\n",
    "        \n",
    "    # convert the dataset to a numpy array\n",
    "    dataset_array = dataset.values\n",
    "    \n",
    "    # one hot encoder object \n",
    "    one_hot_encoder = OneHotEncoder(dtype=np.int64, handle_unknown='ignore')\n",
    "\n",
    "    # apply the one hot encoder object on the independent variable dataset\n",
    "    encoded_x = one_hot_encoder.fit_transform(dataset_array[:, column_index]).toarray()\n",
    "    \n",
    "    # drop the original column from the dataset\n",
    "    dataset_array = np.delete(dataset_array, column_index, axis = 1)\n",
    "    \n",
    "    # add the new columns to the dataset\n",
    "    dataset_array = np.concatenate((dataset_array, encoded_x), axis = 1)\n",
    "    \n",
    "    \n",
    "    # get the column names of the new columns\n",
    "    encoded_x_column_names = one_hot_encoder.get_feature_names_out(input_features=column_names_to_convert)\n",
    "    \n",
    "    # drop the old column from the dataset\n",
    "    dataset = dataset.drop(column_names_to_convert, axis = 1)\n",
    "    \n",
    "    # record the data types of each column\n",
    "    original_data_types = dataset.dtypes.to_dict()\n",
    "    # all the data types are int64 for encoded columns\n",
    "    \n",
    "    \n",
    "    # record the last column number of the dataset\n",
    "    last_column_number = len(dataset.columns)\n",
    "    \n",
    "    # reconstruct the new dataset column names\n",
    "    new_column_names = list(dataset.columns[0:last_column_number-1]) + list(encoded_x_column_names)\n",
    "    new_column_names.append(dataset.columns[last_column_number-1])\n",
    "    \n",
    "    # rearrange the columns of the dataset_array \n",
    "    # i.e. bring the dataset_array column with the last column number to the last column number of the new dataset_array\n",
    "    dataset_array = np.concatenate((dataset_array[:, 0:last_column_number-1], dataset_array[:, last_column_number:], dataset_array[:, last_column_number-1:last_column_number]), axis = 1)\n",
    "    \n",
    "    # convert the dataset to a dataframe\n",
    "    # Here the column names are the original column names and the one hot encoded column names\n",
    "    # and the values are the values of the dataset array\n",
    "    dataset = pd.DataFrame(data=dataset_array, columns = new_column_names)\n",
    "    \n",
    "    # restore the original data types of the columns\n",
    "    for column_name in dataset.columns:\n",
    "        if column_name in original_data_types:\n",
    "            dataset[column_name] = dataset[column_name].astype(original_data_types[column_name])\n",
    "        else:\n",
    "            dataset[column_name] = dataset[column_name].astype('int64')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def align_columns(train, test):\n",
    "    # Get missing columns in the training test\n",
    "    missing_cols = set(train.columns) - set(test.columns)\n",
    "\n",
    "    # Add a missing column in test set with default value equal to 0\n",
    "    for c in missing_cols:\n",
    "        test[c] = 0\n",
    "\n",
    "    # Ensure the order of column in the test set is in the same order than in train set\n",
    "    test = test[train.columns]\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "categorical_col_names = ['workclass', 'marital-status', 'relationship', 'occupation', 'race', 'sex', 'native-country']\n",
    "\n",
    "dataset = convert_categorical(dataset, categorical_col_names)\n",
    "\n",
    "test_dataset = convert_categorical(test_dataset, categorical_col_names)\n",
    "\n",
    "dataset, test_dataset = align_columns(dataset, test_dataset)\n",
    "\n",
    "print(dataset.columns)\n",
    "\n",
    "print(dataset.shape)\n",
    "\n",
    "print(test_dataset.shape)\n",
    "\n",
    "print(test_dataset.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale the dataset\n",
    "\n",
    "For scaling the dataset, we use the `minmax_scale` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "0  0.301370  0.044302            0.8       0.02174           0.0   \n",
      "1  0.452055  0.048238            0.8       0.00000           0.0   \n",
      "\n",
      "   hours-per-week  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
      "0        0.397959                       0                     0   \n",
      "1        0.122449                       0                     0   \n",
      "\n",
      "   workclass_ Never-worked  workclass_ Private  ...  \\\n",
      "0                        0                   0  ...   \n",
      "1                        0                   0  ...   \n",
      "\n",
      "   native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
      "0                            0                         0   \n",
      "1                            0                         0   \n",
      "\n",
      "   native-country_ South  native-country_ Taiwan  native-country_ Thailand  \\\n",
      "0                      0                       0                         0   \n",
      "1                      0                       0                         0   \n",
      "\n",
      "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
      "0                                0                              1   \n",
      "1                                0                              1   \n",
      "\n",
      "   native-country_ Vietnam  native-country_ Yugoslavia  income  \n",
      "0                        0                           0       0  \n",
      "1                        0                           0       0  \n",
      "\n",
      "[2 rows x 90 columns]\n",
      "        age    fnlwgt  education-num  capital-gain  capital-loss  \\\n",
      "0  0.109589  0.144430       0.400000           0.0           0.0   \n",
      "1  0.287671  0.051677       0.533333           0.0           0.0   \n",
      "\n",
      "   hours-per-week  workclass_ Federal-gov  workclass_ Local-gov  \\\n",
      "0        0.397959                       0                     0   \n",
      "1        0.500000                       0                     0   \n",
      "\n",
      "   workclass_ Never-worked  workclass_ Private  ...  \\\n",
      "0                        0                   1  ...   \n",
      "1                        0                   1  ...   \n",
      "\n",
      "   native-country_ Puerto-Rico  native-country_ Scotland  \\\n",
      "0                            0                         0   \n",
      "1                            0                         0   \n",
      "\n",
      "   native-country_ South  native-country_ Taiwan  native-country_ Thailand  \\\n",
      "0                      0                       0                         0   \n",
      "1                      0                       0                         0   \n",
      "\n",
      "   native-country_ Trinadad&Tobago  native-country_ United-States  \\\n",
      "0                                0                              1   \n",
      "1                                0                              1   \n",
      "\n",
      "   native-country_ Vietnam  native-country_ Yugoslavia  income  \n",
      "0                        0                           0       0  \n",
      "1                        0                           0       0  \n",
      "\n",
      "[2 rows x 90 columns]\n"
     ]
    }
   ],
   "source": [
    "# function to scale the dataset\n",
    "def scale_numerical_values(dataset, column_names_to_scale):\n",
    "    for column_name in column_names_to_scale:\n",
    "        # max and min values\n",
    "        max_value = dataset[column_name].max()\n",
    "        min_value = dataset[column_name].min()\n",
    "        \n",
    "        # scale the values\n",
    "        dataset[column_name] = (dataset[column_name] - min_value)/(max_value - min_value)\n",
    "    return dataset\n",
    "\n",
    "numerical_col_names = ['age', 'fnlwgt', 'education-num', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
    "\n",
    "dataset = scale_numerical_values(dataset, numerical_col_names)\n",
    "\n",
    "test_dataset = scale_numerical_values(test_dataset, numerical_col_names)\n",
    "\n",
    "print(dataset.head(2))\n",
    "\n",
    "print(test_dataset.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide the dataset into dependent and independent variables\n",
    "\n",
    "- The dependent variable is `income`\n",
    "- Rest of the variables are independent variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32561, 89)\n",
      "(32561,)\n",
      "[0.30136986 0.0443019  0.8        0.02174022 0.         0.39795918\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         1.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.        ]\n",
      "0\n",
      "(16281, 89)\n",
      "(16281,)\n",
      "[0.10958904 0.14443012 0.4        0.         0.         0.39795918\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 1.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         1.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         1.         0.         0.         0.         1.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         0.         0.         0.         0.\n",
      " 0.         0.         1.         0.         0.        ]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# function to divide the dataset into dependent and independent variables\n",
    "\n",
    "def divide_dataset(dataset):\n",
    "    # divide the dataset into x and y\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "\n",
    "    x = dataset.iloc[:, 0:(dataset_columns_length-1)].values\n",
    "    y = dataset.iloc[:, (dataset_columns_length-1)].values\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x_train, y_train = divide_dataset(dataset)\n",
    "\n",
    "x_test, y_test = divide_dataset(test_dataset)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(x_train[0])\n",
    "print(y_train[0])\n",
    "\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "\n",
    "print(x_test[0])\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "- We use the `CustomLogisticRegression` class from `custom_logistic_regression` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.6921680563465841 \t\n",
      "loss: 0.4026967250859418 \t\n",
      "0.8173330876481789\n",
      "[[11524   911]\n",
      " [ 2063  1783]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.93      0.89     12435\n",
      "           1       0.66      0.46      0.55      3846\n",
      "\n",
      "    accuracy                           0.82     16281\n",
      "   macro avg       0.76      0.70      0.72     16281\n",
      "weighted avg       0.80      0.82      0.81     16281\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import the logistic regression class\n",
    "\n",
    "from custom_logistic_regression import CustomLogisticRegression\n",
    "\n",
    "\n",
    "# create an object of the class\n",
    "classifier = CustomLogisticRegression(early_stopping_threshold=0.01, learning_rate=0.01, num_iterations=20000, verbose=True, num_features=30)\n",
    "\n",
    "# fit the model to the training data\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "# predict the test set results\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "# print the accuracy\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
