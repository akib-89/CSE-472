{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the Datasets\n",
    "\n",
    "We will use the pandas library to read the datasets. We will use the `read_csv` method to read the csv files in python and assign it to a dataframe object.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "- pandas\n",
    "- matplotlib\n",
    "- numpy\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "To install the libraries, you can use `pip` or `conda` as follows:\n",
    "\n",
    "```bash\n",
    "pip install pandas matplotlib scikit-learn numpy\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda install pandas matplotlib scikit-learn numpy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "This dataset can be found on the following [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n",
    "\n",
    "The dataset contains 7043 rows and 21 columns.\n",
    "\n",
    "The dataset contains information about a telecom company that has a problem with the churn rate of its customers. The company wants to know which customers are likely to churn next month. The dataset contains information about:\n",
    "\n",
    "- Customers who left within the last month – the column is called Churn\n",
    "- Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "- Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "- Demographic info about the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('Telco-Customer-Churn.csv')\n",
    "# dataset = pd.read_csv('short.csv')\n",
    "\n",
    "# replace the cells with single and extra spaces with a single space\n",
    "dataset = dataset.replace(r'\\s+', ' ', regex=True)\n",
    "\n",
    "# replace the single space with a NaN value\n",
    "dataset = dataset.replace(r'^\\s*$', np.nan, regex=True)\n",
    "\n",
    "# drop the id column from the dataset\n",
    "dataset = dataset.drop('customerID', axis=1)\n",
    "\n",
    "# How many rows and columns are in this dataset?\n",
    "print(dataset.shape)\n",
    "\n",
    "# Store the column of data types in a list called data_types\n",
    "data_types = dataset.dtypes\n",
    "\n",
    "# Print the data type of each column\n",
    "print(data_types)\n",
    "\n",
    "# What is the first entry in the dataset?\n",
    "print(dataset.head(2))\n",
    "\n",
    "# What is the last entry in the dataset?\n",
    "print(dataset.tail(2))\n",
    "\n",
    "# What is the name of each column?\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the yes and no values to 1 and 0\n",
    "\n",
    "We first will scan for those columns that have yes and no values and convert them to 1 and 0 respectively.\n",
    "\n",
    "**Note:** This cell is dependent on the dataset and the column names. If you are using a different dataset, you will need to change the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(dataset)\n",
    "\n",
    "# Name of the column that needs to be encoded in 0s and 1s\n",
    "column_name = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "\n",
    "# Name of the column that is a categorical variable\n",
    "categorical_col_names = ['Gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "\n",
    "\n",
    "# Create a label encoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# Apply the label encoder object on the data\n",
    "dataset[column_name] = dataset[column_name].apply(le.fit_transform)\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "# print(dataset.head(5))\n",
    "\n",
    "print(dataset.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset into dependent and independent variables\n",
    "\n",
    "We will divide the dataset into dependent and independent variables. \n",
    "\n",
    "The dependent variable is the variable that we want to predict.The independent variables are the variables that we will use to predict the dependent variable. In this case, \n",
    "\n",
    "- It is the `Churn` column. \n",
    "- All the other columns except the `Churn` column and `Customer ID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide the dataset into x and y\n",
    "dataset_columns_length = len(dataset.columns)\n",
    "print(dataset_columns_length)\n",
    "\n",
    "x = dataset.iloc[:, 0:(dataset_columns_length-1)].values\n",
    "y = dataset.iloc[:, (dataset_columns_length-1)].values\n",
    "\n",
    "# What is the shape of x and y?\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# What is the value of the 1st row of x?\n",
    "print(x[0])\n",
    "\n",
    "# What is the value of the 1st row of y?\n",
    "print(y[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "\n",
    "We will check for missing values in the dataset. If there are any missing values, we will replace them with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()   # check for missing values\n",
    "\n",
    "# Store the column number of the columns with missing values in a list called missing_cols\n",
    "missing_cols = [i for i in range(dataset_columns_length) if dataset.iloc[:, i].isnull().any()]\n",
    "print(missing_cols)\n",
    "\n",
    "# Print columns index and names with missing values and the number of missing values\n",
    "for i in missing_cols:\n",
    "    print(i, dataset.columns[i], dataset.iloc[:, i].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "\n",
    "# impute missing values in the columns with missing values\n",
    "# Here the hard coded values are the column numbers of the columns with missing values \n",
    "x[:, 18] = imputer.fit_transform(x[:, 18].reshape(-1, 1)).ravel()\n",
    "\n",
    "# print(x[488])\n",
    "\n",
    "# print the max and min values of the column with missing values\n",
    "print(max(x[:, 18]))\n",
    "print(min(x[:, 18]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re check for the datatypes of the each column in x, y\n",
    "\n",
    "We need to recheck the datatypes of the columns in x and y. If there are any columns that are not numeric, we will convert them to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# length of the independent variable dataset\n",
    "x_length = len(x[0])\n",
    "\n",
    "# find the list of missing columns in the independent variable dataset\n",
    "missing_cols_x = [i for i in range(x_length) if pd.isnull(x[:, i]).any()]\n",
    "\n",
    "print(missing_cols_x)\n",
    "\n",
    "# Print columns index and names with missing values and the number of missing values\n",
    "for i in missing_cols_x:\n",
    "    print(i, dataset.columns[i], pd.isnull(x[:, i]).sum())\n",
    "    \n",
    "    \n",
    "# data types of the independent variable dataset\n",
    "# for each column of independent variable dataset, print the data type\n",
    "for i in range(x_length):\n",
    "    print(x[:, i])\n",
    "    \n",
    "y_data_types = y.dtype\n",
    "\n",
    "# Print the data type of each column\n",
    "print(y_data_types)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for the categorical columns\n",
    "\n",
    "- We need to check for the categorical columns in the dataset.\n",
    "- For this we will use the `one hot encoding` technique. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the list of the columns with the categorical data in the independent variable dataset\n",
    "# here the value is hard coded\n",
    "categorical_col_index = [0, 6, 7, 8, 9, 10, 11, 12, 13, 14, 16]\n",
    "\n",
    "# one hot encoder object \n",
    "# the column names should be passed as a parameter\n",
    "one_hot_encoder = OneHotEncoder(dtype=np.int64, handle_unknown='ignore')\n",
    "\n",
    "# apply the one hot encoder object on the independent variable dataset\n",
    "encoded_x = one_hot_encoder.fit_transform(x[:, categorical_col_index]).toarray()\n",
    "\n",
    "print(one_hot_encoder.get_feature_names_out(input_features=categorical_col_names))\n",
    "\n",
    "# print the first row of the independent variable dataset\n",
    "# print(x[0])\n",
    "print(encoded_x[0])\n",
    "\n",
    "print(encoded_x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now we need to drop the original columns and add the new columns to the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the categorical columns from the independent variable dataset\n",
    "x = np.delete(x, categorical_col_index, axis=1)\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "# concatenate the encoded columns with the independent variable dataset\n",
    "x = np.concatenate((x, encoded_x), axis=1)\n",
    "\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
