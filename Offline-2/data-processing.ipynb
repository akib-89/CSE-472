{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working on the Datasets\n",
    "\n",
    "We will use the pandas library to read the datasets. We will use the `read_csv` method to read the csv files in python and assign it to a dataframe object.\n",
    "\n",
    "### Required Libraries\n",
    "\n",
    "- pandas\n",
    "- matplotlib\n",
    "- numpy\n",
    "- scikit-learn\n",
    "\n",
    "\n",
    "To install the libraries, you can use `pip` or `conda` as follows:\n",
    "\n",
    "```bash\n",
    "pip install pandas matplotlib scikit-learn numpy\n",
    "```\n",
    "\n",
    "```bash\n",
    "conda install pandas matplotlib scikit-learn numpy\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the dataset\n",
    "\n",
    "This dataset can be found on the following [link](https://www.kaggle.com/datasets/blastchar/telco-customer-churn).\n",
    "\n",
    "The dataset contains 7043 rows and 21 columns.\n",
    "\n",
    "The dataset contains information about a telecom company that has a problem with the churn rate of its customers. The company wants to know which customers are likely to churn next month. The dataset contains information about:\n",
    "\n",
    "- Customers who left within the last month – the column is called Churn\n",
    "- Services that each customer has signed up for – phone, multiple lines, internet, online security, online backup, device protection, tech support, and streaming TV and movies\n",
    "- Customer account information – how long they’ve been a customer, contract, payment method, paperless billing, monthly charges, and total charges\n",
    "- Demographic info about the customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load data\n",
    "def load_data(file_name):\n",
    "    # load dataset\n",
    "    dataset = pd.read_csv(file_name)\n",
    "    \n",
    "    # drop the customerID column\n",
    "    dataset.drop('customerID', axis = 1, inplace = True)\n",
    "    \n",
    "    # replace the cells with single and extra spaces with a Nan value\n",
    "    dataset.replace(r'^\\s*$', np.nan, regex=True, inplace = True)\n",
    "    \n",
    "    # convert the TotalCharges column to numeric\n",
    "    dataset['TotalCharges'] = pd.to_numeric(dataset['TotalCharges'])\n",
    "    \n",
    "    # convert the dataset to dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = load_data('Telco-Customer-Churn.csv')\n",
    "\n",
    "# How many rows and columns are in this dataset?\n",
    "print(dataset.shape)\n",
    "\n",
    "# Store the column of data types in a list called data_types\n",
    "data_types = dataset.dtypes\n",
    "\n",
    "# Print the data type of each column\n",
    "print(data_types)\n",
    "\n",
    "# What is the first entry in the dataset?\n",
    "print(dataset.head(2))\n",
    "\n",
    "# What is the last entry in the dataset?\n",
    "print(dataset.tail(2))\n",
    "\n",
    "# What is the name of each column?\n",
    "print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for missing values\n",
    "\n",
    "We will check for missing values in the dataset. If there are any missing values, we will replace them with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.isnull().sum()   # check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find the missing columns\n",
    "def find_missing_columns(dataset):\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    # Store the column number of the columns with missing values in a list called missing_cols\n",
    "    missing_cols = [i for i in range(dataset_columns_length) if dataset.iloc[:, i].isnull().any()]\n",
    "    \n",
    "    # Print columns index and names with missing values and the number of missing values\n",
    "    for i in missing_cols:\n",
    "        print(i, dataset.columns[i], dataset.iloc[:, i].isnull().sum())\n",
    "            \n",
    "    return missing_cols\n",
    "\n",
    "# Store the column number of the columns with missing values in a list called missing_cols\n",
    "missing_cols = find_missing_columns(dataset)\n",
    "\n",
    "print(missing_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def impute_missing_values(dataset, missing_columns):\n",
    "    for column in missing_columns:\n",
    "        column_name = dataset.columns[column]\n",
    "        if dataset[column_name].dtype == 'object' or dataset[column_name].dtype == 'int64':\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='most_frequent')\n",
    "            dataset[column_name] = imputer.fit_transform(dataset[column_name].values.reshape(-1, 1)).ravel()\n",
    "        elif dataset[column_name].dtype == 'float64':\n",
    "            imputer = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "            dataset[column_name] = imputer.fit_transform(dataset[column_name].values.reshape(-1, 1)).ravel()\n",
    "    return dataset\n",
    "\n",
    "dataset = impute_missing_values(dataset, missing_cols)\n",
    "\n",
    "missing_cols = find_missing_columns(dataset)\n",
    "print(missing_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the yes and no values to 1 and 0\n",
    "\n",
    "We first will scan for those columns that have yes and no values and convert them to 1 and 0 respectively.\n",
    "\n",
    "**Note:** This cell is dependent on the dataset and the column names. If you are using a different dataset, you will need to change the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the column that needs to be encoded in 0s and 1s\n",
    "column_name = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling', 'Churn']\n",
    "\n",
    "# function to convert the yes and no values to 1 and 0\n",
    "def convert_yes_no(dataset, column_name_to_convert):\n",
    "    # create a label encoder object\n",
    "    label_encoder = LabelEncoder()\n",
    "    \n",
    "    # Apply label encoder on the column and replace the column with the encoded values\n",
    "    dataset[column_name_to_convert] = dataset[column_name_to_convert].apply(label_encoder.fit_transform)\n",
    "    \n",
    "    # convert the dataset to a dataframe\n",
    "    dataset = pd.DataFrame(dataset)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = convert_yes_no(dataset, column_name)\n",
    "\n",
    "# Print the first 5 rows of the dataset\n",
    "# print(dataset.head(5))\n",
    "\n",
    "print(dataset.dtypes)\n",
    "print(dataset.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting the categorical values to numerical values\n",
    "\n",
    "Now we will convert the categorical values to numerical values. For this purpose we will use the `OneHotEncoder` class from the `sklearn.preprocessing` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the column that is a categorical variable\n",
    "categorical_col_names = ['Gender', 'MultipleLines', 'InternetService', 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies', 'Contract', 'PaymentMethod']\n",
    "\n",
    "# function to convert the categorical values to numerical values using one hot encoding\n",
    "def convert_categorical(dataset, column_names_to_convert):\n",
    "    column_index = []\n",
    "    \n",
    "    # find the index of the categorical columns\n",
    "    for column_name in column_names_to_convert:\n",
    "        column_index.append(dataset.columns.get_loc(column_name))\n",
    "        \n",
    "    # convert the dataset to a numpy array\n",
    "    dataset_array = dataset.values\n",
    "    \n",
    "    # one hot encoder object \n",
    "    one_hot_encoder = OneHotEncoder(dtype=np.int64, handle_unknown='ignore')\n",
    "\n",
    "    # apply the one hot encoder object on the independent variable dataset\n",
    "    encoded_x = one_hot_encoder.fit_transform(dataset_array[:, column_index]).toarray()\n",
    "    \n",
    "    # drop the original column from the dataset\n",
    "    dataset_array = np.delete(dataset_array, column_index, axis = 1)\n",
    "    \n",
    "    # add the new columns to the dataset\n",
    "    dataset_array = np.concatenate((dataset_array, encoded_x), axis = 1)\n",
    "    \n",
    "    \n",
    "    # get the column names of the new columns\n",
    "    encoded_x_column_names = one_hot_encoder.get_feature_names_out(input_features=column_names_to_convert)\n",
    "    \n",
    "    # drop the old column from the dataset\n",
    "    dataset = dataset.drop(column_names_to_convert, axis = 1)\n",
    "    \n",
    "    # record the data types of each column\n",
    "    original_data_types = dataset.dtypes.to_dict()\n",
    "    # all the data types are int64 for encoded columns\n",
    "    \n",
    "    \n",
    "    # record the last column number of the dataset\n",
    "    last_column_number = len(dataset.columns)\n",
    "    \n",
    "    # reconstruct the new dataset column names\n",
    "    new_column_names = list(dataset.columns[0:last_column_number-1]) + list(encoded_x_column_names)\n",
    "    new_column_names.append(dataset.columns[last_column_number-1])\n",
    "    \n",
    "    # rearrange the columns of the dataset_array \n",
    "    # i.e. bring the dataset_array column with the last column number to the last column number of the new dataset_array\n",
    "    dataset_array = np.concatenate((dataset_array[:, 0:last_column_number-1], dataset_array[:, last_column_number:], dataset_array[:, last_column_number-1:last_column_number]), axis = 1)\n",
    "    \n",
    "    # convert the dataset to a dataframe\n",
    "    # Here the column names are the original column names and the one hot encoded column names\n",
    "    # and the values are the values of the dataset array\n",
    "    dataset = pd.DataFrame(data=dataset_array, columns = new_column_names)\n",
    "    \n",
    "    # restore the original data types of the columns\n",
    "    for column_name in dataset.columns:\n",
    "        if column_name in original_data_types:\n",
    "            dataset[column_name] = dataset[column_name].astype(original_data_types[column_name])\n",
    "        else:\n",
    "            dataset[column_name] = dataset[column_name].astype('int64')\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "dataset = convert_categorical(dataset, categorical_col_names)\n",
    "\n",
    "print(dataset.dtypes)\n",
    "# print(dataset.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset into dependent and independent variables\n",
    "\n",
    "We will divide the dataset into dependent and independent variables. \n",
    "\n",
    "The dependent variable is the variable that we want to predict.The independent variables are the variables that we will use to predict the dependent variable. In this case, \n",
    "\n",
    "- It is the `Churn` column. \n",
    "- All the other columns except the `Churn` column and `Customer ID` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to divide the dataset into x and y\n",
    "def divide_dataset(dataset):\n",
    "    # divide the dataset into x and y\n",
    "    dataset_columns_length = len(dataset.columns)\n",
    "    print(dataset_columns_length)\n",
    "\n",
    "    x = dataset.iloc[:, 0:(dataset_columns_length-1)].values\n",
    "    y = dataset.iloc[:, (dataset_columns_length-1)].values\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "x, y = divide_dataset(dataset)\n",
    "# What is the shape of x and y?\n",
    "print(x.shape)\n",
    "print(y.shape)\n",
    "\n",
    "\n",
    "# What is the value of the 1st row of x?\n",
    "print(x[0])\n",
    "\n",
    "# What is the value of the 1st row of y?\n",
    "print(y[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
