{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal compolent analysis\n",
    "\n",
    "### Introduction\n",
    "\n",
    "PCA is a technique for dimensionality reduction. It is used to reduce the number of features in a dataset while retaining the most important information. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the data from the given file.\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "def read_data(filename):\n",
    "    df = pd.read_csv(filename, sep=',', header=None)\n",
    "    return df\n",
    "\n",
    "input_file = 'data/6D_data_points.txt'\n",
    "\n",
    "df = read_data(input_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Print some properties of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "print(df.dtypes)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement PCA\n",
    "\n",
    "Steps:\n",
    "1. Normalize the data\n",
    "1. Compute the covariance matrix\n",
    "1. Compute the eigenvalues and eigenvectors of the covariance matrix\n",
    "1. Sort the eigenvalues and eigenvectors\n",
    "1. Select the first k eigenvectors\n",
    "1. Transform the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PCA(X, reduced_dim):\n",
    "    # mean center the data\n",
    "    X = X - np.mean(X, axis=0)\n",
    "    # calculate the covariance matrix\n",
    "    cov_mat = np.cov(X, rowvar=False)\n",
    "    # calculate eigenvalues and eigenvectors of the covariance matrix\n",
    "    eigen_values, eigen_vectors = np.linalg.eigh(cov_mat)\n",
    "    # sort the eigenvalues in descending order\n",
    "    sorted_index = np.argsort(eigen_values)[::-1]\n",
    "    # sort the eigenvectors according to the sorted eigenvalues\n",
    "    eigen_vectors = eigen_vectors[:, sorted_index]\n",
    "    # select the first n eigenvectors, n is desired dimension\n",
    "    eigen_vectors = eigen_vectors[:, 0:reduced_dim]\n",
    "    # transform the data\n",
    "    X = np.dot(X, eigen_vectors)\n",
    "    return X\n",
    "\n",
    "def PCA_sklearn(X, reduced_dim):\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=reduced_dim)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    return X_reduced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values\n",
    "X_reduced = PCA(X, 2)\n",
    "X_reduced_sklearn = PCA_sklearn(X, 2)\n",
    "\n",
    "# print the shape of the reduced data\n",
    "print(X_reduced.shape)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Plot the reduced data\n",
    "axs[0].scatter(X_reduced[:, 0], X_reduced[:, 1])\n",
    "axs[0].set_title('Data points in 2D')\n",
    "axs[0].set_xlabel('x')\n",
    "axs[0].set_ylabel('y')\n",
    "\n",
    "# Plot the reduced data using sklearn\n",
    "axs[1].scatter(X_reduced_sklearn[:, 0], X_reduced_sklearn[:, 1])\n",
    "axs[1].set_title('Data points in 2D using sklearn')\n",
    "axs[1].set_xlabel('x')\n",
    "axs[1].set_ylabel('y')\n",
    "\n",
    "# Display the figure with subplots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Models\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Gaussian Mixture Models (GMMs) are a probabilistic model for representing normally distributed subpopulations within an overall population.\n",
    "\n",
    "In this exercise, we will use GMMs to perform clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "class GMM:\n",
    "    def __init__(self, k, max_iter=5, verbose=False):\n",
    "        self.k = k\n",
    "        self.max_iter = int(max_iter)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def initialize(self, X):\n",
    "        self.shape = X.shape\n",
    "        self.n, self.m = self.shape\n",
    "\n",
    "        self.phi = np.full(shape=self.k, fill_value=1/self.k)\n",
    "        self.weights = np.full( shape=self.shape, fill_value=1/self.k)\n",
    "        \n",
    "        random_row = np.random.randint(low=0, high=self.n, size=self.k)\n",
    "        self.mu = [  X[row_index,:] for row_index in random_row ]\n",
    "        self.sigma = [ np.cov(X.T) + np.eye(self.m) * 1e-6 for _ in range(self.k) ]\n",
    "\n",
    "    def e_step(self, X):\n",
    "        # E-Step: update weights and phi holding mu and sigma constant\n",
    "        self.weights = self.predict_proba(X)\n",
    "        self.phi = self.weights.mean(axis=0)\n",
    "    \n",
    "    def m_step(self, X):\n",
    "        # M-Step: update mu and sigma holding phi and weights constant\n",
    "        for i in range(self.k):\n",
    "            weight = self.weights[:, [i]]\n",
    "            total_weight = weight.sum()\n",
    "            self.mu[i] = (X * weight).sum(axis=0) / total_weight\n",
    "            self.sigma[i] = np.cov(X.T, \n",
    "                aweights=(weight/total_weight).flatten(), \n",
    "                bias=True)\n",
    "            # add a small variance to avoid numerical instability\n",
    "            self.sigma[i] += np.eye(self.m) * 1e-6\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.initialize(X)\n",
    "        \n",
    "        old_log_likelihood = 0\n",
    "        self.log_likelihoods = []  # Store log-likelihoods for each iteration\n",
    "        self.mus = []  # Store mus for each iteration\n",
    "        self.sigmas = []  # Store sigmas for each iteration\n",
    "        \n",
    "        for iteration in range(self.max_iter):\n",
    "            self.e_step(X)\n",
    "            self.m_step(X)\n",
    "            \n",
    "            # Store the intermediate results\n",
    "            self.log_likelihoods.append(np.sum(np.log(np.sum(self.weights, axis=1))))\n",
    "            self.mus.append(self.mu.copy())\n",
    "            self.sigmas.append(self.sigma.copy())\n",
    "            \n",
    "            # Calculate the log-likelihood\n",
    "            new_log_likelihood = self.log_likelihoods[-1]\n",
    "            \n",
    "            # Check for convergence\n",
    "            if np.abs(new_log_likelihood - old_log_likelihood) < 1e-20:\n",
    "                if self.verbose:\n",
    "                    print(f'Converged after {iteration} iterations.')\n",
    "                break\n",
    "            \n",
    "            old_log_likelihood = new_log_likelihood\n",
    "            \n",
    "    def log_likelihood(self, X):\n",
    "        # return log-likelihood of the model\n",
    "        likelihood = np.zeros( (self.n, self.k) )\n",
    "        for i in range(self.k):\n",
    "            distribution = multivariate_normal(\n",
    "                mean=self.mu[i], \n",
    "                cov=self.sigma[i])\n",
    "            likelihood[:,i] = distribution.pdf(X)\n",
    "        log_likelihood = np.log(np.sum(likelihood, axis=1))\n",
    "        return np.sum(log_likelihood)\n",
    "            \n",
    "    def predict_proba(self, X):\n",
    "        likelihood = np.zeros( (self.n, self.k) )\n",
    "        for i in range(self.k):\n",
    "            distribution = multivariate_normal(\n",
    "                mean=self.mu[i], \n",
    "                cov=self.sigma[i])\n",
    "            likelihood[:,i] = distribution.pdf(X)\n",
    "        \n",
    "        numerator = likelihood * self.phi\n",
    "        denominator = numerator.sum(axis=1)[:, np.newaxis]\n",
    "        weights = numerator / denominator\n",
    "        return weights\n",
    "    \n",
    "    def predict(self, X):\n",
    "        weights = self.predict_proba(X)\n",
    "        return np.argmax(weights, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "def draw_ellipse(position, covariance, ax=None,num_ellipse=4, **kwargs):\n",
    "    \"\"\"Draw an ellipse with a given position and covariance\"\"\"\n",
    "    ax = ax or plt.gca()\n",
    "    # Convert covariance to principal axes\n",
    "    if covariance.shape == (2, 2):\n",
    "        U, s, Vt = np.linalg.svd(covariance)\n",
    "        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n",
    "        width, height = 2 * np.sqrt(s)\n",
    "    else:\n",
    "        angle = 0\n",
    "        width, height = 2 * np.sqrt(covariance)\n",
    "    # Draw the Ellipse\n",
    "    for nsig in range(1, num_ellipse + 1):\n",
    "        ax.add_patch(Ellipse(xy=position, width=nsig * width, height=nsig * height, angle=angle, fill=False, **kwargs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "\n",
    "def generate_plots(gmm, X, K):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    def update(i):\n",
    "        ax.clear()\n",
    "        ax.scatter(X[:, 0], X[:, 1], c=gmm.predict(X))\n",
    "        for j in range(K):\n",
    "            draw_ellipse(gmm.mus[i][j], gmm.sigmas[i][j], alpha=0.5)\n",
    "        ax.set_title(f'Iteration {i+1}')\n",
    "    \n",
    "    ani = animation.FuncAnimation(fig, update, frames=len(gmm.log_likelihoods), repeat=True)\n",
    "    \n",
    "    return ani"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the range for K\n",
    "K_range = range(3, 9)\n",
    "\n",
    "# Initialize the list to store the best log-likelihoods\n",
    "best_log_likelihoods = []\n",
    "\n",
    "best_gmms = []\n",
    "\n",
    "\n",
    "# For each value of K\n",
    "for K in K_range:\n",
    "    best_log_likelihood = float('-inf')\n",
    "    best_gmm = None\n",
    "    \n",
    "    # Run the EM algorithm five times\n",
    "    for _ in range(5):\n",
    "        gmm = GMM(K, max_iter=100, verbose=False)\n",
    "        gmm.fit(X_reduced)\n",
    "        \n",
    "        # Compute the log-likelihood\n",
    "        epsilon = 1e-15\n",
    "        log_likelihood = gmm.log_likelihood(X_reduced)\n",
    "        \n",
    "        # Update the best log-likelihood if necessary\n",
    "        if log_likelihood > best_log_likelihood:\n",
    "            best_log_likelihood = log_likelihood\n",
    "            best_gmm = gmm\n",
    "    \n",
    "    # Store the best log-likelihood for this value of K\n",
    "    best_log_likelihoods.append(best_log_likelihood)\n",
    "    \n",
    "    # Store the best GMM for this value of K\n",
    "    best_gmms.append(best_gmm)\n",
    "    \n",
    "    \n",
    "    plt.figure()\n",
    "    # add a scatter plot of the data points\n",
    "    plt.scatter(X_reduced[:, 0], X_reduced[:, 1], c=best_gmm.predict(X_reduced))\n",
    "    # add the ellipses\n",
    "    for i in range(K):\n",
    "        draw_ellipse(best_gmm.mu[i], best_gmm.sigma[i], alpha=0.5)\n",
    "    # add a title        \n",
    "    plt.title(f'K = {K}')\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "\n",
    "# Plot the best log-likelihoods\n",
    "plt.plot(K_range, best_log_likelihoods)\n",
    "plt.title('Best Log-Likelihood vs. Number of Components')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Best Log-Likelihood')\n",
    "plt.show()\n",
    "\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the animation\n",
    "for gmm in best_gmms:\n",
    "    ani = generate_plots(gmm, X_reduced, K)\n",
    "    ani.save(f'gmm_{K}.gif', writer='imagemagick', fps=1)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
